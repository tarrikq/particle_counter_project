---
title: "particleCounterBefore06172021"
author: "Tarrik Quneibi"
date: "8/9/2021"
output: html_document
---




```{r installPackages, include=FALSE, eval=FALSE, echo=FALSE}
    ## echo false = don't show code in knitted doc
    ##include false = don't include the chunk in the doc after running
    ##eval false = don't run code in the chunk 

##install packages
install.packages("ggplot2")
install.packages("reshape2")
install.packages("dplyr")
install.packages("tidyr")
install.packages("chron")
install.packages("anytime")
install.packages("lubridate")
install.packages("tidyverse")
install.packages("bayesbio")



```


```{r openLibraries, eval=TRUE, echo=TRUE, include=FALSE}
##setting the working directory
##setwd("`/ R code/ particle counter data/")

##setwd("C:/Users/RLahr/OneDrive - City of Ann Arbor/RHL/data/R_data analysis/Tarrik/Particle counter code_7.1.2021")

library(ggplot2)
library(reshape2)
library(dplyr)
library(tidyr)
library(chron)
library(anytime)
library(lubridate)
library(tidyverse)
library(bayesbio)

```

# Data Processing

## 1) List the files

Calling in all files that end in .csv.

Creating empty lists for the particle data(table_p), flow data (table_d), and file names for both (p_titles, f_titles). 

creating vectors for the column names which will be applied after each dataframe is cleaned.
```{r listCreation, include=TRUE, eval=TRUE, echo=TRUE}


##adds all .csv files in the working directory into a list
file_list <- list.files(pattern = ".csv")

##creating empty lists to later add variables into
table_p <-list()
table_f <- list()
p_titles <- list()
f_titles <- list()
file_titles <- list()

##list of column names for particle data and flow data
p_columns <- c("Date","Bin1", "Bin2", "Bin3", "Bin4", "Bin5", "Bin6", "Bin7", "Bin8", "Bin9","Filter" , "sampledDatePC")
f_columns <- c("Date", "Flow", "Turbidity", "Filter", "sampledDateFlow", "sampledFlow")

```

## 2) load particle count files

This loop will take in only files from the file_list that do not have the word "Flow" in the file name. This is how the particle counter data will be found.

The loop begins by changing the name of the file so it is easier to read and then stores the name in the vector (p_titles)

The file is then read and the columns of interest (columns 2-12) are kept while the rest is removed.

The date and time columns are then combined into one column and changed to date-time class. The old time column is deleted.

The date column is then rounded to the nearest 15 minute interval to match the flow data.

The data and file name are combined and placed into table_p to create a list of all the files.

Outside of the for loop, the list of files are iterated through to remove any null entries and the file name is then applied to each dataframe.

```{r particle counter, include=TRUE, eval=TRUE, echo=TRUE}

##iteration variable
i <- 0
##loop through directory to pull out all particle data
for (file in file_list){
  i=i+1
 if(!grepl("Flow", file)){
   
##removes the .csv and date from the end of each file name and stores the name in a list
    titles <- substring(file, 1, nchar(file) - 13)
    p_titles[[i]] <- titles
    
##reading in the file and taking only the important columns
    p_data <- read.csv(file)
    p_data <- p_data[, 2:12]
    p_data$filter <- substring(titles, 7, 8)
    
##combines the date and time columns back into a single column with the correct format. 
##Also changes the class from CHAR to Date-Time
    p_data$Date <- as.POSIXct(paste(p_data$Date, p_data$Time), tz = "EST", format="%m/%d/%Y %H:%M")
    p_data <- subset( p_data, (Date >= as.Date("2021-02-01") & Date <= as.Date("2021-06-17")))
##deleting the old time column
    p_data[ , c('Time')] <- list(NULL)
    
##the time was rounded to the nearest 15 minute mark so that it could be accurately joined with the flow data. 
    ##But keep the old time info that wasn't rounded so you know when the particle counter data was recorded. 
    p_data$sampledDatePC <- p_data$Date
    p_data$Date <- lubridate::round_date(p_data$Date, unit = "15 minutes")
       
    
##combines the file name with the data associated with it to create a variable
    
    colnames(p_data) <- p_columns
    assign(titles, p_data)
    table_p <- append(table_p, list(p_data))
    names(table_p[i]) <- titles
    
 }}


##removes all the null entries in both lists and then names each data frame by its file name
table_p <- table_p[-which(lapply(table_p,is.null) == T)]
p_titles <- p_titles[-which(lapply(p_titles,is.null) == T)]
names(table_p) <- p_titles

```

```{r seperating PC filter data}
pcTitles <- list()
pcList <- list()
i <- 0
for (filter in table_p){
  i <- i+1
   titles <- substring(filter, 1, length(nchar(filter)))
   pcTitles[[i]] <- names(table_p[i])
   filter$Filter <- as.numeric(filter$Filter)
   
      if (nrow(filter) == 0){
      print("There is no particle counter data")
      pcList[[i]] <- filter
      next
    } 
   
      if (filter$Filter[1] == 13 || filter$Filter[1] == 14){
        filter$group <- format(filter$Date,"%H:%M:%S")
        filter$group <- substring(filter$group, 4, 5)
        filterNew <- subset(filter, (group == "15" | group == "45"))
        filter <- anti_join(filter, filterNew)
       filterNew$Filter <- filterNew$Filter + 2
       pcList[[i]] <- filter
        i <- i +1
        pcList[[i]] <- filterNew
        
      }
      if (filter$Filter[1] == 11){
       filter$group <- format(filter$Date,"%H:%M:%S")
        filter$group <- substring(filter$group, 4, 5)
        filterNew <- subset(filter, (group == "15" | group == "45"))
        filter <- anti_join(filter, filterNew)
        filterNew$Filter <- filterNew$Filter - 2
        pcList[[i]] <- filter
        i <- i +1
        pcList[[i]] <- filterNew
        }
      if (filter$Filter[1] == 12 || filter$Filter[1] == 17 || filter$Filter[1] == 21 || filter$Filter[1] == 18){
        pcList[[i]] <- filter
            }
      if (filter$Filter[1] == 23){
        filter$Filter <- filter$Filter +1
        pcList[[i]] <- filter
             }
      if (filter$Filter[1] == 25){
       filter$group <- format(filter$Date,"%H:%M:%S")
        filter$group <- substring(filter$group, 4, 5)
        filterNew <- subset(filter, (group == "15" | group == "45"))
        filter <- anti_join(filter, filterNew)
        filterNew$Filter <- filterNew$Filter + 1
        pcList[[i]] <- filter
        i <- i +1
        pcList[[i]] <- filterNew
            }
}
allPCData <- bind_rows(pcList)

```

## 3) load flow files

This loop will take in only files from the file_list that have the word "Flow" in the file name. This is how the flow data will be found.

The loop begins by changing the name of the file so it is easier to read and then stores the name in the vector (f_titles)

The file is then read and the columns of interest (columns 1-3) are kept while the rest is removed.

Every 3rd row is then taken and the rest removed to have a data frame for each 15 minute interval (rather than every 5 minutes)

The date and time columns are then split into two columns and formatted to match the particle counter data (MDY)

The date and time columns are then combined into one column and changed to date-time data.

The date column is then rounded to the nearest 15 minute interval to match the flow data.

The flow data column is then rounded to either 1 or 0 depending if it is greater than or less than 0.01

The data and file name are combined and placed into table_f to create a list of all the files.

Outside of the for loop, the list of files are iterated through to remove any null entries and the file name is then applied to each dataframe.

```{r flowData, include=TRUE, eval=TRUE, echo=TRUE}
    ## echo false = don't show code in knitted doc
    ##include false = don't include the chunk in the doc after running
    ##eval false = don't run code in the chunk 

##iteration variable
i <- 0

##loop to pull all flow data from the directory
for (file in file_list){
  i=i+1
  if(grepl("Flow|flow", file)){
    
##removes the .csv from the end of each file name
   titles <- substring(file, 1, nchar(file) - 13)
   f_titles[i] <- titles
   
   
##reading each file
   f_data <- read.csv(file)
   f_data <- f_data[ ,1:3]
   f_data$filter <- substring(titles, 7, 8)
##takes every third row so that only every 15 minutes is considered. 
   #Do we need to? Left_join shoudl be ablet to handle the flow data as every 5 minutes and just sort out the numbers needed. 
   #f_data = f_data[seq(1, nrow(f_data), 3), ]
   
##changes the first column name to date
   colnames(f_data)[1] <- "Date"

##Tarrik: The time data wasn't converting properly for the time...so instead used the parse_date_time function to convert it and stored it as sampledDateFlow. Needed a date time not just a time. 
   f_data$sampledDateFlow <- parse_date_time(f_data$Date, tz = "EST", orders = "mdyHM", truncated = 3)

   
##separates the date and time into two columns
   
   f_data <- f_data %>% separate(Date, c("Date", "Time"), " ")
   
#changes the date to a new format which matches the particle counter date format
   f_data$Date <- as.Date(f_data$Date, format = "%m/%d/%y")
   
   
##combines the date and time columns back into a single column with the correct format. 
##Also changes the class from CHAR to Date-Time
    ##this next one not working....also didn't work to just use the sampledDateFlow. so convert the sampledDateFlow into as.POSIXct
   #f_data$Date <- as.POSIXct(paste(f_data$Date, f_data$Time), format="%m/%d/%Y %H:%M")
   
      f_data$Date <- as.POSIXct(f_data$sampledDateFlow, tz = "EST", format="%m/%d/%Y %H:%M")
   
    
   
   
##deleting the old time column
   f_data[ , c('Time')] <- list(NULL)
   
##the time was rounded to the nearest 15 minute mark so that it could be accurately joined with the particle data
      ##should be able to get by with rounding every 5 min... left_join should be able to match that. 
   f_data$Date <- round_date(f_data$Date, unit = "5 minutes")

   
##takes any flow data value above 0.01 and changes it to 1, while anything below 0.01 is changed to 0
    f_data$sampledFlow <- f_data[ , 2]
   f_data[ , 2] <- ifelse(f_data[ ,2] > 0.01, 1, 0)
   
##combines the file name with the data associated with it to create a variable
   colnames(f_data) <- f_columns
   assign(titles, f_data)
   table_f <- append(table_f, list(f_data))
   names(table_f[i]) <- titles
  }
}

##removes all the null entries in both lists and then names each data frame by its file name
table_f <- table_f[-which(lapply(table_f,is.null) == T)]
f_titles <- f_titles[-which(lapply(f_titles,is.null) == T)]
names(table_f) <- f_titles

allFlowData <- bind_rows(table_f)
allFlowData$Filter <- as.numeric(allFlowData$Filter)
```

## 4) Join the data together into one list with a dataframe for each filter. 

This will join the particle counter and flow data into on dataframe based on the date and time.

The two dataframes it will look at is determined by how the files are listed in the directory.

Be sure to list the files alphabetically so that R brings in the PC and flow data in the same order based on filter number.

The loop will then multiple the flow data by he PC data to find the relevent data.

It ends by adding in the columns that did not need multiplied.

```{r joining, include=TRUE, eval=TRUE, echo=TRUE}
    ## echo false = don't show code in knitted doc
    ##include false = don't include the chunk in the doc after running
    ##eval false = don't run code in the chunk 
  
  allData <- left_join(allPCData, allFlowData, by = c("Date", "Filter"))
   Date <- allData$Date
   flow <- allData$Flow
   turbidity <- allData$Turbidity
   sampledDateFlow<- allData$sampledDateFlow
   sampledDatePC<- allData$sampledDatePC
   sampledFlow <- allData$sampledFlow
   filter <- allData$Filter
   allData <- (allData[ , 2:10])*(allData[ , 14])
   allData <- cbind(turbidity, allData) 
   allData <- cbind(flow, allData) 
   allData <- cbind(sampledDateFlow, allData)
   allData <- cbind(sampledDatePC, allData)
   allData <- cbind(sampledFlow, allData)
   allData <- cbind(Date, allData)
   allData <- cbind(filter, allData)

   pcList <- split(allData , f = allData$filter )
   titles <- names(pcList)

```
  
## 5) Break data into runs

This code begins by subsetting each dataframe to find only the times where flow is above 0.1

It then determines and assignes the run number based on the amount of time that has passed between data points.

It ends by placing all of the updated dataframes into a list by filter number.

```{r filterRuns, include=TRUE, eval=TRUE, echo=TRUE}

   list_p <- list()
   i <- 0
   z <- 0
   p_list <- list()
  for (df in pcList){
   z <- z+1
   j <- 1
    df.name <- deparse(substitute(df))
    sub <- subset(df, flow >0.01)
    
    if (nrow(sub) == 0){
      print("There is no relevent data. Check the flow data and particle data dates to see if they match")
      p_list[[z]] <- sub
      next
    }
    
    sub$difference <- 0
    sub$runID <- 0

    
    for (i in 1:length(sub$Date)){
      difference <- abs(difftime(sub$Date[i+1], sub$Date[i] ,units="mins"))
      sub$difference[i] <- difference
      
      ##runDifference <- difftime(sub$Date[i], sub$Date[1], units = "hours")
      ##sub$runningDifference[i] <- runDifference
      if (is.na(difference)) {
        difference <- 0
      }
      if (i == 1 & difference < 200){
        sub$runID[i] <- 1
        sub$runID[i+1] <- 1
      }
      if ( i == length(sub$Date)){
        sub$runID[i] <- j
      }
      if (i == 1 & difference > 200){
        j <- j+1
        sub$runID[i] <- 1
        sub$runID[i+1] <- j
      }
      
      
     if (difference < 200 & i != 1 & i != length(sub$Date)) {
      sub$runID[i+1] <- j
      
      
    } 
    if ( difference > 200 & i != 1 & i != length(sub$Date)){
     j <- j+1
      sub$runID[i+1] <- j
   }
  
    }
    p_list[[z]] <- sub

  }
       names(p_list) <- titles
    
    #then repeat all the code above but find the next filterOnDate AFTER the filterOffDate...and append the data into filterRunPCs.Do this until run out dates.   
    
    #then stored in a list like table_p where each filter has one dataframe.  
    
    
    #then once that code all works, put it into a for loop so it would run for every filter. Will need to instead start with an empty filterRunPCs dataframe and append to it instead of just staring it fresh.  

```

## 6) Combining into one dataframe

This loop splits each data frame from the list by their runs. 

It then adds a columns for total run time and adds the new dataframe into a list.

The end result is a list of dataframes based on filter number and run ID.

It then combines all dataframes from the list into a single dataframe.


```{r adding run time and combining into one dataframe}
list_df <-list()
i <- 0
z <- 0
for (df in p_list){
  i <- i+1
  listDF <- split(df, df$runID) 
  
  for (runs in listDF){
    z <- z+1
    
    for (j in 1:nrow(runs)){
      runs$runDifference[j] <- difftime(runs$Date[j], runs$Date[1], units = "hours")
    }
    
  p_list[[z]] <- runs  
  }
}
allData <- bind_rows(p_list)
write.csv(allData,"C:/Users/Tarri/Desktop/PCcountsBefore06172021.csv", row.names = FALSE)

```

## 7) Plotting all data based on runtime intervals

This code will subset the main dataframe to find the datapoints that occur at specified run time intervals.

It will then plot thos points on a scatterplot.

The first scatter plot shows all of the data, some of which is from a different month than the rest.

The second plot limits the data points to show just the month of july and some of august 2021.

```{r plotting 95 percent confidence interval}


min15 <- subset(allData, runDifference == 0.25)

min30 <- subset(allData, runDifference == 0.50)

hr24 <- subset(allData, runDifference == 24.00)

hr50 <- subset(allData, runDifference == 50.00)


ggplot(min15, aes(x=Bin1)) + geom_histogram(binwidth=5) 
ggplot(min30, aes(x=Bin1)) + geom_histogram(binwidth=5) 
ggplot(hr24, aes(x=Bin1)) + geom_histogram(binwidth=5) 
ggplot(hr50, aes(x=Bin1)) + geom_histogram(binwidth=5) 

```
